{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "175e77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import urduhack\n",
    "import pandas as pd\n",
    "from urduhack.normalization import normalize\n",
    "from urduhack.preprocessing import normalize_whitespace, remove_punctuation, remove_accents, replace_urls, replace_emails, replace_currency_symbols\n",
    "from typing import FrozenSet\n",
    "import stanfordnlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score\n",
    "from feature_selection import FeatureSelection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88716107",
   "metadata": {},
   "source": [
    "### 1. Import training and test data for Urdu fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff3f1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = []\n",
    "train_label = []\n",
    "\n",
    "files = glob.glob(\"../datasets/fake-new-corpus/Train/Real/*.txt\", recursive = True) \n",
    "\n",
    "for each in files:\n",
    "    file = open(each, encoding = \"UTF-8\")\n",
    "    text = file.read()\n",
    "    train_news.append(text)\n",
    "    train_label.append(0)\n",
    "    \n",
    "files = glob.glob(\"../datasets/fake-new-corpus/Train/Fake/*.txt\", recursive = True) \n",
    "\n",
    "for each in files:\n",
    "    file = open(each, encoding = \"UTF-8\")\n",
    "    text = file.read()\n",
    "    train_news.append(text)\n",
    "    train_label.append(1)\n",
    "    \n",
    "# conver to dataframe\n",
    "training_df = pd.DataFrame({'text': train_news, 'label': train_label})\n",
    "\n",
    "#shuffle the dataframe\n",
    "training_df = shuffle(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcaf762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = []\n",
    "test_label = []\n",
    "\n",
    "files = glob.glob(\"../datasets/fake-new-corpus/Test/Real/*.txt\", recursive = True) \n",
    "\n",
    "for each in files:\n",
    "    file = open(each, encoding = \"UTF-8\")\n",
    "    text = file.read()\n",
    "    test_news.append(text)\n",
    "    test_label.append(0)\n",
    "    \n",
    "files = glob.glob(\"../datasets/fake-new-corpus/Test/Fake/*.txt\", recursive = True) \n",
    "\n",
    "for each in files:\n",
    "    file = open(each, encoding = \"UTF-8\")\n",
    "    text = file.read()\n",
    "    test_news.append(text)\n",
    "    test_label.append(1)\n",
    "\n",
    "# store to a dataframe\n",
    "\n",
    "test_df = pd.DataFrame({'text': test_news, 'label': test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f869c71",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "072aefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # remove the html tags\n",
    "    text = re.sub('<.*?>', '', text) \n",
    "\n",
    "    # remove punctuation marks and numbers\n",
    "    #text = re.sub(r\"[^a-zA-Z@]+\", ' ', text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # remove new line character\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('،', ' ')\n",
    "    \n",
    "    # remove English stopwords\n",
    "    stop_words = ['AFP', 'Image', 'caption', 'Reuters', 'EPA', 'AFPGetty', 'getty', \n",
    "                  'Getty', 'Images', 'AFP/Getty Images', 'AFP/Getty', 'Sunday', 'Monday', 'Wednesday', 'July']\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    final_text = []\n",
    "    \n",
    "    for word in words: \n",
    "        if word:\n",
    "            if not word in stop_words:\n",
    "                final_text.append(word)\n",
    "    \n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be9ea2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply urduhack normalization on training/test data\n",
    "training_df.text = training_df.text.apply(normalize).apply(remove_punctuation).apply(remove_accents).apply(replace_urls).apply(replace_emails).apply(replace_currency_symbols).apply(normalize_whitespace)\n",
    "test_df.text = test_df.text.apply(normalize).apply(remove_punctuation).apply(remove_accents).apply(replace_urls).apply(replace_emails).apply(replace_currency_symbols).apply(normalize_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a54286d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Urdu stopwords\n",
    "STOP_WORDS: FrozenSet[str] = frozenset(\"\"\"\n",
    " آ آئی آئیں آئے آتا آتی آتے آس آمدید آنا آنسہ آنی آنے آپ آگے آہ آہا آیا اب ابھی ابے\n",
    " ارے اس اسکا اسکی اسکے اسی اسے اف افوہ البتہ الف ان اندر انکا انکی انکے انہوں انہی انہیں اوئے اور اوپر\n",
    " اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکثر اگر اگرچہ اہاہا ایسا ایسی ایسے ایک بائیں بار بارے بالکل باوجود باہر\n",
    " بج بجے بخیر بشرطیکہ بعد بعض بغیر بلکہ بن بنا بناؤ بند بڑی بھر بھریں بھی بہت بہتر تاکہ تاہم تب تجھ\n",
    " تجھی تجھے ترا تری تلک تم تمام تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے تیرا تیری تیرے\n",
    " جا جاؤ جائیں جائے جاتا جاتی جاتے جانی جانے جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی جیسا\n",
    " جیسوں جیسی جیسے حالانکہ حالاں حصہ حضرت خاطر خالی خواہ خوب خود دائیں درمیان دریں دو دوران دوسرا دوسروں دوسری دوں\n",
    " دکھائیں دی دیئے دیا دیتا دیتی دیتے دیر دینا دینی دینے دیکھو دیں دیے دے ذریعے رکھا رکھتا رکھتی رکھتے رکھنا رکھنی\n",
    " رکھنے رکھو رکھی رکھے رہ رہا رہتا رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی\n",
    " سراسر سمیت سوا سوائے سکا سکتا سکتے سہ سہی سی سے شاید شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور علاوہ عین\n",
    " فقط فلاں فی قبل قطا لئے لائی لائے لاتا لاتی لاتے لانا لانی لانے لایا لو لوجی لوگوں لگ لگا لگتا\n",
    " لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن لیں لیے لے ماسوا مت مجھ مجھی مجھے محترم محترمہ محض\n",
    " مرا مرحبا مری مرے مزید مس مسز مسٹر مطابق مل مکرمی مگر مگھر مہربانی میرا میروں میری میرے میں نا نزدیک\n",
    " نما نہ نہیں نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ وغیرہ ولے وگرنہ وہ وہاں\n",
    " وہی وہیں ویسا ویسے ویں پاس پایا پر پس پلیز پون پونی پونے پھر پہ پہلا پہلی پہلے پیر پیچھے چاہئے\n",
    " چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ چکی چکیں چکے ڈالنا ڈالنی ڈالنے ڈالے کئے کا کاش کب کبھی\n",
    " کدھر کر کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس کسی کسے کم کن کنہیں کو کوئی کون کونسا\n",
    " کونسے کچھ کہ کہا کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے کے گئی گئے گا گنا\n",
    " گو گویا گی گیا ہائیں ہائے ہاں ہر ہرچند ہرگز ہم ہمارا ہماری ہمارے ہمی ہمیں ہو ہوئی ہوئیں ہوئے ہوا\n",
    " ہوبہو ہوتا ہوتی ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\n",
    "      \"\"\".split())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join(word for word in text.split() if word not in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e5964f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(text):\n",
    "    return text.replace(\"\\ufeff\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cbd87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and any special characters\n",
    "training_df.text = training_df.text.apply(preprocess)\n",
    "training_df.text = training_df.text.apply(remove_stopwords)   \n",
    "training_df.text = training_df.text.apply(remove_special_char) \n",
    "\n",
    "\n",
    "test_df.text = test_df.text.apply(preprocess)\n",
    "test_df.text = test_df.text.apply(remove_stopwords)   \n",
    "test_df.text = test_df.text.apply(remove_special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aebc0d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>اڈیلیڈ ڈسمبر سیاست ڈاٹ کام ہندوستانی کرکٹ ٹیم ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>آنکھیں توجہ طالب آنکھ نازک عضو صحت خاص احتیاط ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>پاکستان درآمد بطور تحائف دئیے ٹیلی فونز بلاک ج...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cnnurdu شائع نئی دہلی بھارتی ویمن ٹی ٹوئنٹی کر...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>تصویر کاپی رائٹ سرفراز احمد رنز آؤٹ ہوگئے انڈی...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "217  اڈیلیڈ ڈسمبر سیاست ڈاٹ کام ہندوستانی کرکٹ ٹیم ...      0\n",
       "115  آنکھیں توجہ طالب آنکھ نازک عضو صحت خاص احتیاط ...      0\n",
       "596  پاکستان درآمد بطور تحائف دئیے ٹیلی فونز بلاک ج...      1\n",
       "210  cnnurdu شائع نئی دہلی بھارتی ویمن ٹی ٹوئنٹی کر...      0\n",
       "243  تصویر کاپی رائٹ سرفراز احمد رنز آؤٹ ہوگئے انڈی...      0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2990a",
   "metadata": {},
   "source": [
    "### 3. Tokenization and binary vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f2a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'D:\\\\stanfordnlp_resources\\\\ur_udtb_models\\\\ur_udtb_tokenizer.pt', 'lang': 'ur', 'shorthand': 'ur_udtb', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# configure the stanford nlp tokenizer for urdu\n",
    "config = {\n",
    "    'processors': 'tokenize', # Comma-separated list of processors to use\n",
    "    'lang': 'ur', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': r'D:\\stanfordnlp_resources\\ur_udtb_models\\ur_udtb_tokenizer.pt'\n",
    "}\n",
    "\n",
    "nlp = stanfordnlp.Pipeline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc9aa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tokenizer function\n",
    "def tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        tokens += [f\"{token.text}\" for token in sentence.tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf3a2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to binary vectors\n",
    "vectorizer = CountVectorizer(binary=True, tokenizer=tokenizer)\n",
    "\n",
    "vectorized_train_x = vectorizer.fit_transform(training_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49b71d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(638, 12496)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71174542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the test set as well\n",
    "vectorized_test_x = vectorizer.transform(test_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0688a",
   "metadata": {},
   "source": [
    "### 4. Feature ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd688149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features based on different feature ranking metrics\n",
    "fs = FeatureSelection()\n",
    "\n",
    "# the results can be reproduced with different feature selection measure and different value of k\n",
    "selector = SelectKBest(score_func=partial(fs.IG, vectorizer = vectorizer), k=1500)\n",
    "new_train_x = selector.fit_transform(vectorized_train_x, training_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7982e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_x = selector.transform(vectorized_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "723e5ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6789667896678966\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(new_train_x, training_df.label)\n",
    "\n",
    "y_hat = clf.predict(new_test_x.toarray())\n",
    "f1 = f1_score(test_df.label, y_hat)\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d0877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
